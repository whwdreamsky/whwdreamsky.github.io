<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Decision Tree motivation : 一方面好奇这个看似简单的基于规则的机器学习算法的神奇之处，一方面是Semval 13 baseline 甚至state of art 方法采用的，还有机器学习 need to learn   历史重要人物：Quinlan（罗斯.昆兰） Hunt：  ID3： C4.5： CART: 引入回归树，应对输出Y是连续变量的回归问题，同时使用Gini">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树">
<meta property="og:url" content="http://yoursite.com/2018/04/30/DescionTree/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Decision Tree motivation : 一方面好奇这个看似简单的基于规则的机器学习算法的神奇之处，一方面是Semval 13 baseline 甚至state of art 方法采用的，还有机器学习 need to learn   历史重要人物：Quinlan（罗斯.昆兰） Hunt：  ID3： C4.5： CART: 引入回归树，应对输出Y是连续变量的回归问题，同时使用Gini">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-04-29T16:15:56.680Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树">
<meta name="twitter:description" content="Decision Tree motivation : 一方面好奇这个看似简单的基于规则的机器学习算法的神奇之处，一方面是Semval 13 baseline 甚至state of art 方法采用的，还有机器学习 need to learn   历史重要人物：Quinlan（罗斯.昆兰） Hunt：  ID3： C4.5： CART: 引入回归树，应对输出Y是连续变量的回归问题，同时使用Gini">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/04/30/DescionTree/"/>





  <title>决策树 | Hexo</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/04/30/DescionTree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">决策树</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-30T00:13:27+08:00">
                2018-04-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h1><blockquote>
<p>motivation : 一方面好奇这个看似简单的基于规则的机器学习算法的神奇之处，一方面是Semval 13 baseline 甚至state of art 方法采用的，还有机器学习 need to learn </p>
</blockquote>
<h3 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h3><p>重要人物：<strong>Quinlan（罗斯.昆兰）</strong></p>
<p><strong>Hunt</strong>： </p>
<p><strong>ID3：</strong></p>
<p><strong>C4.5：</strong></p>
<p>CART: 引入回归树，应对输出Y是连续变量的回归问题，同时使用Gini 指数作为分类树的划分原则</p>
<p><a href="http://blog.csdn.net/lc013/article/details/55048641" target="_blank" rel="noopener">对各种决策树描述</a></p>
<hr>
<h2 id="决策树学习（西瓜书）"><a href="#决策树学习（西瓜书）" class="headerlink" title="决策树学习（西瓜书）"></a>决策树学习（西瓜书）</h2><h3 id="决策树基本思想"><a href="#决策树基本思想" class="headerlink" title="决策树基本思想"></a>决策树基本思想</h3><p>ID3 算法：</p>
<p>决策树有一个根节点，一系列的非叶子节点，叶子节点构成，其实决策树本质是利用非叶子节点中对属性不断测试，不断选择属性再根据属性值对数据进行划分，从数据中学习到一系列的规则最为分类的依据，决策树的生成是一个==递归==的过程，下图是对决策树基本的算法的总结：</p>
<p>==TODO python 实现==</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#以下是决策树基本流程的伪代码</span></span><br><span class="line"><span class="comment">#input type:</span></span><br><span class="line"><span class="comment">#数据集:D((x1,y1),(x2,y2).....)</span></span><br><span class="line"><span class="comment">#A 属性列表A = [a1,a2,....]</span></span><br><span class="line">Tree tree;</span><br><span class="line">Node node;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TreeGenerate</span><span class="params">(D,A)</span>:</span></span><br><span class="line">    produceNode()</span><br><span class="line">    tempset = set(D[<span class="number">1</span>,:])</span><br><span class="line">    <span class="keyword">if</span> len(tempset)==<span class="number">1</span>:</span><br><span class="line">        setNode(tree,tempset[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment">#isSameValue 判断D中所有条目A属性值都相同</span></span><br><span class="line">    <span class="keyword">if</span> len(A)==<span class="number">0</span>||isSameValue(D,A)==<span class="keyword">True</span>:</span><br><span class="line">        y = D[:,<span class="number">1</span>]</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        ydict = &#123;&#125;</span></span><br><span class="line"><span class="string">        for item in set(y):</span></span><br><span class="line"><span class="string">            ydict[item] = y.count(item)</span></span><br><span class="line"><span class="string">        '''</span> </span><br><span class="line">        setNode(tree,findmaxFreq(y))</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    a = findOptimizeAtr(A)</span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> A:</span><br><span class="line">        dv = D[D[:,<span class="number">0</span>].has(a)]</span><br><span class="line">        <span class="keyword">if</span> len(dv)==<span class="number">0</span>:</span><br><span class="line">            setNode(tree,findmaxFreq(D[:,]))</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            TreeGenerate(dv,A.remove(a))</span><br></pre></td></tr></table></figure>
<h3 id="最优属性选择问题"><a href="#最优属性选择问题" class="headerlink" title="最优属性选择问题"></a>最优属性选择问题</h3><p>问题：findOptimizeAtr(A) 要选择最优划分属性</p>
<ul>
<li><p>信息增益的方法</p>
<ul>
<li><p>信息熵的概念:信息熵就是平均而言一个事件发生得到的信息量大小，也就是信息量的期望值</p>
<p>$$<br>\begin{eqnarray<em>}<br>Ent(D) = -\sum_{k = 1}^{|y|}p_klog_2p_k\tag{1.1} \<br>Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{D_v}{D}Ent(D_v) \tag{1.2} \end{eqnarray</em>}<br>$$</p>
<blockquote>
<p>Ent ： 这里y 代表的是分类的类别数，Ent(D) 我理解是每个节点的信息熵，信息的含义就是分类为y个类别的纯度，类别越集中熵值越小</p>
<p>Gain ： V 代表的是属性a 的值的个数，也就是当前父节点的分支个数，这里也就是算出每个子节点的信息熵再加权取均值，再和父节点的信息熵做差值，这就叫信息增益，ID3 算法也就是优先选择信息增益大的属性</p>
</blockquote>
</li>
<li><p>ID3: 从根节点，计算每个特征下的信息增益，优先选择增益大的属性作为节点特征，并为特征的不同值建立子节点。</p>
</li>
<li><p>缺陷：对属性值多个的有偏好</p>
</li>
</ul>
</li>
<li><p>增益率（C4.5）：特征a对数据集D的信息增益 比 训练集D关于特征a的信息熵 值的熵IV(a)，ps Dv 也就是根据某个属性值a的一个划分<br>$$<br>GainRadio(D,a) = \frac{Gain(D,a)}{IV(a)}\\<br>IV(a ) = -\sum\frac{|D_v|}{|D|}log_2\frac{|D_v|}{|D|}<br>$$</p>
<p>​</p>
<blockquote>
<p>其实增益率也就是克服之前信息增益对属性值多的偏好问题，除了一个IV，这个值当属性值种类越多IV 的值也越大，但是这有带来了对属性值种类数较少的偏好，在C4.5中是用<code>启发</code>的方式，先选择信息增益大于平均水平的的，再从中选增益率大的</p>
</blockquote>
</li>
<li><p>基尼指数（CART）</p>
<p>假设有K 类，样本点属于k 的概率为Pk，则概率分布的基尼指数定义如下，这里pk 由极大似然统计得到，我们可以看到，Gini 指数描述的是数据的不确定性，Gini 越大表示数据越不确定，当p=1/2 时取最大值<br>$$<br>Gini = \sum_kp_k(1-p_k) = 1-\sum p_k^2<br>$$<br>跟之前熵类似，在做属性值选择时，根据比例计算使用该属性以及各个属性值划分之后的基尼指数,假设在D属性下，属性值为D1，D2<br>$$<br>Gini(D,A)  = \frac{D_1}{D}Gini(D_1)+\frac{D_2}{D}Gini(D_2)<br>$$<br>​</p>
<p>​</p>
</li>
</ul>
<h4 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h4><p>回归树：之前输入变量（X，Y），Y 往往是要预测的某个类别，而有时需要Y 是个连续的值，以用于回归任务中</p>
<p>预测误差：均方误差 ，把决策树构造出的函数设为 f(x) ，那误差就是 $\sum (y_i-f(x_i))^2$）,这样每个叶子节点的最优值是输入实例的均值</p>
<p>寻找最佳属性和划分点：选择第j 个属性和它取的值s，作为切分变量和切分属性，这样就把空间 划分为 x&lt;=s . x&gt;s ,这样我们加合两部分的损失就是我们的选择属性的损失，通过选择 不同的 j，s 获得最小的损失<br>$$<br>loss = min_{j,s}[\min_{c1}\sum_{x&lt;s}(y_i-c_1)^2+\min_{c2}\sum_{x&gt;=s}(y_i-c_2)^2]<br>$$</p>
<blockquote>
<p>这里c1 就是所有预测y 的均值，$c_1=ave(y_i|x&lt;s)$,这里是为了计算均方误差</p>
</blockquote>
<h3 id="属性值划分问题"><a href="#属性值划分问题" class="headerlink" title="属性值划分问题"></a>属性值划分问题</h3><ul>
<li><p>连续值划分</p>
<p>二分法：找到属性所有取值中信息增益最大的作为划分点，这里属性划分后还可作为后代节点的划分属性</p>
<p>计算每个值得基尼指数或者熵选择最优的</p>
<blockquote>
<p>这里有疑问，如果这样属性就可以被多次划分，理论上可以不断进行划分，知道每个叶子节点只有一条数据，所以在我们就很需要剪枝，但是在后剪枝中应该会因为这个问题，损耗过多的时间和空间</p>
<p>==TODO==</p>
</blockquote>
</li>
<li><p>缺失值划分</p>
<p>要解决的问题：</p>
<ol>
<li><p>属性选择时熵计算</p>
</li>
<li><p>如何对缺失的条目进行划分</p>
<p>利用已知条目的划分来估计缺失条目划分，好处是不需要t填充缺失值</p>
</li>
</ol>
<p>​</p>
<ol>
<li>最常值</li>
<li>​</li>
</ol>
</li>
<li><p>多变量属性值</p>
<p>特定：非叶子节点不再是对某个特定属性进行判断，而是属性的线性组合</p>
</li>
</ul>
<h3 id="过拟合问题优化"><a href="#过拟合问题优化" class="headerlink" title="过拟合问题优化"></a>过拟合问题优化</h3><p>关键问题： 什么时候停止划分</p>
<p>在最基础的hunt 中，在如下情况下停止：</p>
<ol>
<li>输入的数据集D 的类别相同</li>
<li>在属性a的条件b 下 数据集值为空</li>
</ol>
<p>####剪枝</p>
<ul>
<li><p>预剪枝</p>
<p>即在生成决策树的时候就决定是否剪枝</p>
</li>
<li><p>后剪枝</p>
<p>即先生成决策树，再自底向上判断节点剪枝前后对决策树精度的影响。这里注意根据奥卡姆剃刀原则，尽管对精度不产生影响也要剪枝</p>
</li>
</ul>
<p>剪枝的目标函数: 假设叶子节点总数为T，t 为一个叶子节点$N_t$<br>$$<br>C_\alpha (T)=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|<br>$$</p>
<h3 id="随机森林-Random-Forest"><a href="#随机森林-Random-Forest" class="headerlink" title="随机森林 Random Forest"></a>随机森林 Random Forest</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>随机森林：随机森林由许多的决策树组成，因为这些决策树的形成采用了<strong>随机</strong>的方法，所以叫做随机森林，每个决策树的构建没有关系，当进入对测试集进行测试，就是让每一个决策树进行预测，得到多个结果，选择类别最多的</p>
<p>随机森林的诞生是因为决策树容易过拟合</p>
<p>####随机采样</p>
<ul>
<li><p>Bagging: </p>
<ol>
<li>使用Bootstrap 自助法，也就是抽取一定数量样本有放回，生成n个样本子集，</li>
<li>拿每个样本子集独立地训练分类器这样得到m个分类器</li>
<li>预测的时候通过投票法，平均法 对样本进行预测</li>
</ol>
<blockquote>
<p>这里提到从偏差和方差的方面，这种方法降低了方差</p>
</blockquote>
</li>
<li><p>random forest： 随机森林其实就是在bagging 的基础上进一步加入了，在决策树训练时属性选择的随机性，和普通的直接选择全部属性的最优属性不同，随机森林在随机生成的属性子集中选择最优属性</p>
<blockquote>
<p>觉得加入这样的随机策略，目的是之前的思路是让每个分类器有多样性</p>
</blockquote>
</li>
</ul>
<p>###sklearn 实战</p>
<h3 id="参看资料"><a href="#参看资料" class="headerlink" title="参看资料"></a>参看资料</h3><p><a href="http://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener">http://www.cnblogs.com/pinard/p/6050306.html</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/04/29/hello-hexo/" rel="next" title="hello hexo">
                <i class="fa fa-chevron-left"></i> hello hexo
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Decision-Tree"><span class="nav-number">1.</span> <span class="nav-text">Decision Tree</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#历史"><span class="nav-number">1.0.1.</span> <span class="nav-text">历史</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树学习（西瓜书）"><span class="nav-number">1.1.</span> <span class="nav-text">决策树学习（西瓜书）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树基本思想"><span class="nav-number">1.1.1.</span> <span class="nav-text">决策树基本思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最优属性选择问题"><span class="nav-number">1.1.2.</span> <span class="nav-text">最优属性选择问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#回归树"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">回归树</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#属性值划分问题"><span class="nav-number">1.1.3.</span> <span class="nav-text">属性值划分问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过拟合问题优化"><span class="nav-number">1.1.4.</span> <span class="nav-text">过拟合问题优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#随机森林-Random-Forest"><span class="nav-number">1.1.5.</span> <span class="nav-text">随机森林 Random Forest</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#概述"><span class="nav-number">1.1.5.1.</span> <span class="nav-text">概述</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参看资料"><span class="nav-number">1.1.6.</span> <span class="nav-text">参看资料</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
